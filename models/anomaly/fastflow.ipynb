{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc78e233",
   "metadata": {},
   "source": [
    "# FastFlow Anomaly Detection\n",
    "This notebook trains and evaluates a FastFlow model for anomaly detection.  Design tenants include high visibility on image data and model performance.  The goal is to provide a comprehensive framework for developing anomaly detection models, while explaining the code along the way.\n",
    "\n",
    "### Contents\n",
    "1. Setup Variables\n",
    "2. Setup MLFLow Logging\n",
    "3. Data Augmentations\n",
    "4. Data Preparation\n",
    "5. Expand Dataset with Augmentations\n",
    "6. What is a DataLoader and what is a DataModule?\n",
    "7. Visualize Augmentations\n",
    "8. Training the Model\n",
    "9. Model Evaluation\n",
    "10. Visualize a Single Prediction\n",
    "11. Save the Model\n",
    "12. Predict and Visualize the Dataset\n",
    "13. Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7e3c9",
   "metadata": {},
   "source": [
    "## Setup Variables ‚öôÔ∏è\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "load_dotenv(\"../../secrets.env\")\n",
    "load_dotenv(\"../../config.env\")\n",
    "\n",
    "ENVIRONMENT = os.getenv(\"ENVIRONMENT\", \"DEVELOPMENT\")\n",
    "\n",
    "logging.info(f\"Running in {ENVIRONMENT} environment\")\n",
    "\n",
    "if ENVIRONMENT == \"PRODUCTION\":\n",
    "    MLFLOW_URI = os.getenv('MLFLOW_URI')\n",
    "else:\n",
    "    MLFLOW_URI = \"http://mlflow-mlflow\"\n",
    "\n",
    "if MLFLOW_URI is None:\n",
    "    logging.warning(\"MLFLOW_URI not found in environment variables. MLFlow logging will not be enabled.\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket_name\", type=str, default=\"mvtechad\", help=\"S3 bucket name\")\n",
    "parser.add_argument(\"--category\", type=str, default=\"XRFC-PCB-Quality\", help=\"Dataset category (top-level directory in bucket)\")\n",
    "#parser.add_argument(\"--category\", type=str, default=\"bottle\", help=\"Dataset category (top-level directory in bucket)\")\n",
    "parser.add_argument(\"--cache_directory\", type=str, default=\"s3cache\", help=\"Cache directory (local path, will be created if not exists)\")\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=100, help=\"Maximum number of epochs for training\")\n",
    "args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "bucket_name = args.bucket_name\n",
    "category = args.category\n",
    "cache_directory = args.cache_directory\n",
    "max_epochs = args.max_epochs\n",
    "\n",
    "logging.info(f\"Using bucket: {bucket_name}, category: {category}, cache directory: {cache_directory}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4485a",
   "metadata": {},
   "source": [
    "## Setup MLFLow Logging üìä\n",
    "MLFlow lets us log model parameters, metrics, and artifacts, making it easier to track experiments and reproduce results.  Additionally, it provides a centralized repository for managing and comparing different model versions.\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLFlow Logging\n",
    "\n",
    "if MLFLOW_URI is not None:\n",
    "    logging.info(f\"MLFLOW_URI found: {MLFLOW_URI}\")\n",
    "    import mlflow\n",
    "\n",
    "experiment_name = f\"{category}_fastflow_experiment\"\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdc00f",
   "metadata": {},
   "source": [
    "## Data Augmentations üèãÔ∏è\n",
    "\n",
    "Data augmentation is a technique used to increase the diversity of your training dataset by applying various transformations to the existing data. This helps improve the model's generalization and robustness by exposing it to different variations of the input data, especially for smaller datasets.\n",
    "\n",
    "Here is ChatGPT's summary of the Anomalib documentation on the subject:\n",
    "\n",
    "- Model-specific transforms (e.g., resizing, normalization) are tightly coupled to the model architecture and should be set via the model's PreProcessor or configure_pre_processor method. These transforms are included in the exported model graph (ONNX/OpenVINO) and always applied during inference.\n",
    "- Data augmentations (e.g., flips, rotations, brightness/contrast, noise) are for dataset enrichment and should be passed to the datamodule as train_augmentations, val_augmentations, or test_augmentations. These are not included in the exported model and are only applied during training.\n",
    "- Do not mix: Do not add normalization or resizing to augmentations‚Äîthese must be in the model-specific transforms. Likewise, do not add random augmentations to the model-specific transforms, as they will be applied during inference and validation, which is undesirable.\n",
    "- Torchvision v2 API is recommended for augmentations in Anomalib (not Albumentations).\n",
    "- Pitfalls: If you add normalization or resizing to augmentations, you may get double normalization or unexpected image sizes. Always use the correct transform for the correct purpose.\n",
    "\n",
    "Recommended usage for FastFlow:\n",
    "\n",
    "- Use Torchvision v2 transforms for augmentations in the datamodule.\n",
    "- Use the model's default preprocessor for resizing and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "augmentations = v2.Compose([\n",
    "    # v2.RandomHorizontalFlip(p=0.5),\n",
    "    # v2.RandomVerticalFlip(p=0.2),\n",
    "    #v2.RandomRotation(degrees=2),  # always applies\n",
    "    v2.GaussianBlur(kernel_size=(3, 5), sigma=(0.1, 2.0)),\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # always applies\n",
    "    #v2.RandomGrayscale(p=0.1),\n",
    "    v2.RandomApply([v2.RandomAdjustSharpness(sharpness_factor=2)], p=0.3),\n",
    "    # v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # shift up to ¬±10% in x/y\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beced",
   "metadata": {},
   "source": [
    "## Data Preparation ‚¨áÔ∏è\n",
    "\n",
    "We'll use our s3data library to download/cache the data from our S3 bucket. This module expects a directory structure similar to the MVTechAD dataset. Then we'll use the anomalib Folder datamodule to hold our dataset.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3dataset import s3data\n",
    "from anomalib.data import Folder\n",
    "import os\n",
    "\n",
    "s3data.set_aws_config(\n",
    "    access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('AWS_REGION_NAME')\n",
    ")\n",
    "\n",
    "def count_images(directory):\n",
    "    image_exts = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        count += sum(f.lower().endswith(image_exts) for f in files)\n",
    "    return count\n",
    "\n",
    "images, class_names, labels, tags = s3data.get_dataset(bucket_name=bucket_name, prefix=category, limit=1000, cache_dir=cache_directory)\n",
    "\n",
    "test_dir = os.path.join(cache_directory, category, 'test')\n",
    "print(f\"Test directory: {test_dir} with total count: {count_images(test_dir)}\")\n",
    "\n",
    "mask_dir = os.path.join(cache_directory, category, 'ground_truth')\n",
    "print(f\"Mask directory: {mask_dir} with total count: {count_images(mask_dir)}\")\n",
    "\n",
    "train_dir = os.path.join(cache_directory, category, 'train', 'good') # unsupervised training (no labels...only good images)\n",
    "print(f\"Train directory: {train_dir} with total count: {count_images(train_dir)}\")\n",
    "\n",
    "if os.path.exists(test_dir):\n",
    "    test_data_available = True\n",
    "    normal_test_dir = os.path.join('test', 'good') if os.path.exists(os.path.join(test_dir, 'good')) else None\n",
    "\n",
    "    abnormal_dirs = [\n",
    "        os.path.join('test', d)\n",
    "        for d in os.listdir(test_dir)\n",
    "        if os.path.isdir(os.path.join(test_dir, d)) and d != 'good'\n",
    "    ]\n",
    "    print(f\"Found abnormal test directories: {abnormal_dirs} with total count: {count_images(test_dir)}\")\n",
    "\n",
    "    mask_dirs = [\n",
    "        os.path.join('ground_truth', d)\n",
    "        for d in os.listdir(mask_dir)\n",
    "        if os.path.isdir(os.path.join(mask_dir, d))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found mask directories: {mask_dirs} with total count: {count_images(mask_dir)}\")\n",
    "else:\n",
    "    logging.warning(\"No test directories found.\")\n",
    "    test_data_available = False\n",
    "    normal_test_dir = None\n",
    "    abnormal_dirs = None\n",
    "    mask_dirs = None\n",
    "\n",
    "datamodule = Folder(\n",
    "    name=category,\n",
    "    root=os.path.join(cache_directory, category),\n",
    "    normal_dir=os.path.join('train', 'good'), \n",
    "    mask_dir=mask_dirs,\n",
    "    normal_test_dir=normal_test_dir,\n",
    "    abnormal_dir=abnormal_dirs,\n",
    "    train_augmentations=augmentations,  # <-- Apply augmentation to training data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "datamodule.prepare_data()\n",
    "\n",
    "print(f\"Setting up '{datamodule.name}' datasets...\")\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd50bd",
   "metadata": {},
   "source": [
    "## Expand Dataset with Augmentations üêáüêáüêá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationMultiplier(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, augmentations, copies=3):\n",
    "        \"\"\"\n",
    "        Wrapper dataset that returns multiple augmented versions of each sample\n",
    "        \n",
    "        Args:\n",
    "            original_dataset: The base dataset\n",
    "            augmentations: Transformations to apply\n",
    "            copies: Number of augmented copies to create per original image\n",
    "        \"\"\"\n",
    "        self.dataset = original_dataset\n",
    "        self.augmentations = augmentations\n",
    "        self.copies = copies\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.copies\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get original sample\n",
    "        original_idx = idx // self.copies\n",
    "        sample = self.dataset[original_idx]\n",
    "        \n",
    "        # If not the first copy, apply augmentation\n",
    "        if idx % self.copies != 0:\n",
    "            # Handle both tensor images and sample objects\n",
    "            if isinstance(sample, torch.Tensor):\n",
    "                sample = self.augmentations(sample)\n",
    "            elif hasattr(sample, 'image'):\n",
    "                # For anomalib samples\n",
    "                augmented = self.augmentations(sample.image)\n",
    "                if isinstance(augmented, tuple):\n",
    "                    sample.image = augmented[0]\n",
    "                else:\n",
    "                    sample.image = augmented\n",
    "                    \n",
    "        return sample\n",
    "    \n",
    "    @property\n",
    "    def collate_fn(self):\n",
    "        return self.dataset.collate_fn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase dataset size with augmentations\n",
    "\n",
    "print(f\"Original training dataset size: {len(datamodule.train_data)}\")\n",
    "augmentation_copies = 10  # Number of copies to create\n",
    "\n",
    "# Replace the train_data with our augmented version\n",
    "datamodule.train_data = AugmentationMultiplier(\n",
    "    datamodule.train_data, \n",
    "    augmentations,\n",
    "    copies=augmentation_copies\n",
    ")\n",
    "\n",
    "print(f\"Augmented training dataset size: {len(datamodule.train_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44301f3",
   "metadata": {},
   "source": [
    "### What is a DataLoader and what is a DataModule? ü§î\n",
    "\n",
    "**DataLoader:**\n",
    "\n",
    "- A PyTorch `DataLoader` is a utility that loads data from a dataset and serves it in batches to your model during training or inference.\n",
    "- It handles shuffling, batching, and parallel loading using multiple workers.\n",
    "- Example: `DataLoader(dataset, batch_size=32, shuffle=True)`\n",
    "\n",
    "**DataModule:**\n",
    "\n",
    "- A `DataModule` (from PyTorch Lightning or similar frameworks) is a higher-level abstraction that organizes all data-related steps for a project.\n",
    "- It encapsulates dataset preparation, setup, and provides ready-to-use DataLoaders for training, validation, and testing.\n",
    "- This helps keep data logic separate from model logic and makes experiments more reproducible.\n",
    "- Example: `datamodule.train_dataloader()` returns a DataLoader for training data.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for exploring the dataset only, and has no impact on training or the rest of the notebook\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training dataset size: {len(datamodule.train_data)}\")\n",
    "print(f\"Test dataset size: {len(datamodule.test_data)}\")\n",
    "\n",
    "datamodule_attribs = (attr for attr in dir(datamodule) if not attr.startswith(\"_\"))\n",
    "print(\"\\nDatamodule attributes:\")\n",
    "print(list(datamodule_attribs))\n",
    "\n",
    "# Get dataloaders\n",
    "train_loader = datamodule.train_dataloader()\n",
    "test_loader = datamodule.test_dataloader()\n",
    "\n",
    "print(\"\\nDataLoader batch sizes:\")\n",
    "print(f\"Training batch size: {train_loader.batch_size}\")\n",
    "print(f\"Test batch size: {test_loader.batch_size}\")\n",
    "\n",
    "# Get a item or batch\n",
    "data_item = next(iter(train_loader))\n",
    "print(f\"\\nThe dataloader provided the type {type(data_item)}\")\n",
    "data_item_attribs = (attr for attr in dir(data_item) if not attr.startswith(\"_\"))\n",
    "print(f\"Data item attributes: {list(data_item_attribs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ab624",
   "metadata": {},
   "source": [
    "## Visualize Augmentations üé®\n",
    "Visualizing augmentations helps us understand how the transformations affect our images. This is crucial for ensuring that the augmentations are appropriate and beneficial for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single training image and visualize augmentations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_img = data_item.image[0]  # Get the first image in the batch\n",
    "\n",
    "# Create a figure to display augmentations\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    # Apply augmentation (note: augmentations might return just the image if not designed for mask)\n",
    "    # Some augmentations return (img, mask) tuple, others might return just img\n",
    "    augmentation_result = augmentations(sample_img)\n",
    "    \n",
    "    # Handle both cases where augmentation returns tuple or just image\n",
    "    if isinstance(augmentation_result, tuple):\n",
    "        augmented_img = augmentation_result[0]\n",
    "    else:\n",
    "        augmented_img = augmentation_result\n",
    "        \n",
    "    # Visualize the augmented image\n",
    "    axes[i].imshow(augmented_img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Augmentation {i+1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06c76a",
   "metadata": {},
   "source": [
    "## Training the Model üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è\n",
    "- Callbacks are used to log metrics and model checkpoints during training. This allows for stopping training early if the model is not improving.\n",
    "- Model checkpoints allow you to resume training from a specific point or to evaluate the model's performance at different stages.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models import Fastflow\n",
    "from anomalib.engine import Engine\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "# Custom callback to log checkpoints to MLflow\n",
    "import lightning.pytorch as pl\n",
    "class MLflowCheckpointLogger(pl.callbacks.Callback):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        # log the train loss\n",
    "        mlflow.log_metric(\"train_loss\", trainer.callback_metrics[\"train_loss\"].item())\n",
    "        # logging.info(\"Saving checkpoint to MLflow\")\n",
    "        # # Find the last checkpoint path from ModelCheckpoint callback\n",
    "        # for cb in trainer.callbacks:\n",
    "        #     if isinstance(cb, ModelCheckpoint):\n",
    "        #         ckpt_path = cb.last_model_path\n",
    "        #         if ckpt_path and os.path.exists(ckpt_path):\n",
    "        #             mlflow.log_artifact(ckpt_path)\n",
    "\n",
    "model = Fastflow(\n",
    "    # backbone=\"resnet18\",  # or resnet50\n",
    "    backbone=\"wide_resnet50_2\",  # or resnet50\n",
    "    pre_trained=True,\n",
    "    evaluator=test_data_available  #if test_data_available (we found test directories), then we can evaluate performance in training\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"train_loss\", \n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"epoch_{epoch}_step_{step}\",\n",
    "        auto_insert_metric_name=False,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"train_loss\",\n",
    "        patience=3,\n",
    "    ),\n",
    "    MLflowCheckpointLogger(),\n",
    "]\n",
    "\n",
    "engine = Engine(\n",
    "    callbacks=callbacks,\n",
    "    logger=None,  # Disable AnomalibMLFlowLogger since there was a bug with artifact logging. We'll use MLflow directly\n",
    "    accelerator=\"auto\", \n",
    "    devices=1, \n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "mlflow.end_run() #in case there is a run already active \n",
    "mlflow_run = mlflow.start_run(tags={\"model\": \"fastflow\"}) \n",
    "mlflow.log_param(\"category\", category)\n",
    "mlflow.log_param(\"backbone\", \"resnet18\")\n",
    "mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f83bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Training the model\")\n",
    "\n",
    "engine.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Evaluating model\")\n",
    "\n",
    "results = engine.validate(model=model, datamodule=datamodule)\n",
    "\n",
    "metrics_dict = {}\n",
    "if isinstance(results, list) and results and isinstance(results[0], dict):\n",
    "    metrics_dict['image_AUROC'] = float(results[0].get('image_AUROC', float('nan')))\n",
    "    metrics_dict['pixel_AUROC'] = float(results[0].get('pixel_AUROC', float('nan')))\n",
    "\n",
    "if metrics_dict:\n",
    "    logging.info(f\"Skipping MLflow logging of metrics due to unique value constraint and conflict with log_model().\")\n",
    "    # logging.info(f\"Logging metrics to MLflow: {metrics_dict}\")\n",
    "    # mlflow.log_metrics(metrics_dict)\n",
    "else:\n",
    "    logging.warning(\"No metrics found to log. Is the evaluator working properly?\")\n",
    "\n",
    "logging.info(\"Training and evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630281c",
   "metadata": {},
   "source": [
    "# Visualize a Single Prediction üñºÔ∏è\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9183932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# TODO: Implement batching, and create a plots for multiple sample.  These will be logged to MLflow with the model.\n",
    "\n",
    "if test_data_available:\n",
    "    test_samples = datamodule.test_data.samples\n",
    "    random_sample = test_samples.sample(1)\n",
    "    path = str(random_sample.iloc[0][\"image_path\"])\n",
    "else:\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    random_sample = train_loader.dataset.samples.sample(1)\n",
    "    path = str(random_sample.iloc[0][\"image_path\"])\n",
    "\n",
    "predictions = engine.predict(model=model, data_path=path)  #You can also point to a folder with image or a single image instead of passing a dataset\n",
    "\n",
    "def plot_results(data_sample):\n",
    "    path = str(data_sample.iloc[0][\"image_path\"])\n",
    "\n",
    "\n",
    "    original_img = Image.open(path)\n",
    "    if test_data_available:\n",
    "        mask = Image.open(str(random_sample.iloc[0][\"mask_path\"]))\n",
    "    else:\n",
    "        mask = Image.new(\"L\", original_img.size, 0)\n",
    "\n",
    "    # Extract the first prediction result\n",
    "    pred = predictions[0]\n",
    "\n",
    "    print(\n",
    "        f'Image Shape: {pred.image.shape},\\n'\n",
    "        f'Anomaly Map Shape: {pred.anomaly_map.shape}, \\n'\n",
    "        f'Predicted Mask Shape: {pred.pred_mask.shape}',\n",
    "    )\n",
    "\n",
    "    # Extract label and tag from the sample DataFrame\n",
    "    label = random_sample.iloc[0][\"label\"] if \"label\" in random_sample.columns else None\n",
    "    tag = random_sample.iloc[0][\"tag\"] if \"tag\" in random_sample.columns else None\n",
    "\n",
    "    # Create subplots for image, anomaly map, and predicted mask\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    img = pred.image\n",
    "    print(f\"Image Shape: {img.shape}\")\n",
    "    print(f\"Image dims: {img.dim()}\")\n",
    "    if img.dim() == 4:\n",
    "        # Batch of images, select the first one\n",
    "        img = img[0]\n",
    "    if img.dim() == 3:\n",
    "        img = img.permute(1, 2, 0).cpu().numpy()  # reorders the dimensions of the tensor from `[C, H, W]` to `[H, W, C]`\n",
    "    elif img.dim() == 2:\n",
    "        img = img.cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape after batch selection: {img.shape}\")\n",
    "\n",
    "    anomaly_map = pred.anomaly_map\n",
    "    if anomaly_map.dim() > 2:\n",
    "        anomaly_map = anomaly_map[0]\n",
    "    anomaly_map = anomaly_map.squeeze().cpu().numpy()\n",
    "\n",
    "    pred_mask = pred.pred_mask\n",
    "    if pred_mask.dim() > 2:\n",
    "        pred_mask = pred_mask[0]\n",
    "    pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "    title_str = \"Original Image\"\n",
    "    title_str += f\"\\nLabel: {label} | Tag: {tag}\"\n",
    "\n",
    "    axs[0].imshow(original_img)\n",
    "    axs[0].set_title(title_str)\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(anomaly_map, cmap=\"jet\")\n",
    "    axs[1].set_title(\"Anomaly Map\")\n",
    "    axs[2].imshow(pred_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[3].imshow(mask, cmap=\"gray\")\n",
    "    axs[3].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "    for ax in axs: ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Path: {path}\")\n",
    "    print(f\"Pred Score: {pred.pred_score}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "figure = plot_results(random_sample)\n",
    "figure.savefig(\"fastflow_prediction.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66efac7",
   "metadata": {},
   "source": [
    "# Save the Model üíæ\n",
    "Log the model to MLFlow.  In the MLFlow UI we can promote the model to production, or set up automations to promote it automatically based on our collected metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "logging.info(\"Saving the model\")\n",
    "\n",
    "model_name = f\"{category}_fastflow\"\n",
    "model_metrics = {}\n",
    "if 'metrics_dict' in locals() and metrics_dict:\n",
    "    if 'image_AUROC' in metrics_dict:\n",
    "        model_metrics['image_AUROC'] = metrics_dict['image_AUROC']\n",
    "    if 'pixel_AUROC' in metrics_dict:\n",
    "        model_metrics['pixel_AUROC'] = metrics_dict['pixel_AUROC']\n",
    "\n",
    "dataset_info = {\n",
    "    \"train_size\": len(datamodule.train_data),\n",
    "    \"test_size\": len(datamodule.test_data),\n",
    "    \"category\": category,\n",
    "    \"bucket\": bucket_name\n",
    "}\n",
    "\n",
    "# Log the prediction image as an artifact\n",
    "if os.path.exists(\"fastflow_prediction.png\"):\n",
    "    mlflow.log_artifact(\"fastflow_prediction.png\", \"predictions\")\n",
    "\n",
    "logged_model = mlflow.pytorch.log_model(\n",
    "    model,\n",
    "    name=model_name, #artifact_path is depreciated\n",
    "    metadata={\n",
    "        'image_AUROC': model_metrics.get('image_AUROC'),\n",
    "        'pixel_AUROC': model_metrics.get('pixel_AUROC'),\n",
    "        'dataset_train_size': dataset_info.get('train_size'),\n",
    "        'dataset_test_size': dataset_info.get('test_size'),\n",
    "        'dataset_bucket_name': dataset_info.get('bucket'),\n",
    "        'dataset_category': dataset_info.get('category')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Register the model in MLflow Model Registry\n",
    "registered_model = mlflow.register_model(\n",
    "    logged_model.model_uri,\n",
    "    f\"{category}_fastflow_model\"\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(f\"Model and artifacts registered as: {registered_model.name} (Version: {registered_model.version})\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cc9c2",
   "metadata": {},
   "source": [
    "## Predict and Visualize the Dataset üìÖ\n",
    "We will use the trained FastFlow model to make predictions on the entire dataset and visualize the results using Voxel FiftyOne. With this interactive tool, we can explore the model's predictions, view the original images, and analyze the results.  The goal is to gain insights into the model's performance and identify any potential areas for improvement, both in the model and the dataset.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2563d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fiftyone = False  # Set to False to skip FiftyOne visualization\n",
    "if not use_fiftyone:\n",
    "    exit()\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone.core.expressions import ViewField as F\n",
    "from fiftyone.core.labels import Classification, Segmentation\n",
    "import numpy as np\n",
    "\n",
    "images, class_names, labels, tags = s3data.get_dataset(bucket_name=bucket_name, prefix=category, limit=1000, cache_dir=cache_directory)\n",
    "# Normalize all image paths to absolute paths for consistent lookup\n",
    "def norm_path(p):\n",
    "    return os.path.normcase(os.path.abspath(p))\n",
    "\n",
    "dataset_dict_by_path = {\n",
    "    norm_path(img): {\n",
    "        \"image\": img,\n",
    "        \"class_name\": class_names[i],\n",
    "        \"label\": labels[i],\n",
    "        \"tags\": tags[i]\n",
    "    }\n",
    "    for i, img in enumerate(images)\n",
    "}\n",
    "\n",
    "print(\"Sample ground truth labels:\")\n",
    "for i, (path, data) in enumerate(list(dataset_dict_by_path.items())[:50]):\n",
    "    print(f\"{i}: {path} ‚Üí Label: {data['label']}\")\n",
    "\n",
    "print(f\"Total images to process: {len(images)}\")\n",
    "# Create FO dataset from images, class_names, labels, and tags\n",
    "name = \"my-labelled-dataset\"\n",
    "if fo.dataset_exists(name):\n",
    "    fo.delete_dataset(name)\n",
    "dataset = fo.Dataset(name=name)\n",
    "\n",
    "predictions = engine.predict(model=model, datamodule=datamodule)  # use the test data\n",
    "#predictions = engine.predict(model=model, dataloaders=train_loader)  # use the training data \n",
    "print(f\"Batches of predictions made: {len(predictions)}\")\n",
    "# flatten batches\n",
    "predictions = [item for batch in predictions for item in batch]\n",
    "print(f\"Total predictions made: {len(predictions)}\")\n",
    "\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "\n",
    "    ## Important prediction attributes:\n",
    "    # image (Image)\n",
    "    # anomaly_map (Mask)\n",
    "    # pred_score (tensor([[0.09]]))\n",
    "    # pred_mask (Mask)\n",
    "    # pred_label (tensor[False]))\n",
    "    # image_path (['C:/...'])\n",
    "\n",
    "    anomaly_map = pred.anomaly_map\n",
    "    pred_score = float(pred.pred_score) if hasattr(pred, 'pred_score') else None\n",
    "    pred_mask = pred.pred_mask\n",
    "    image_path = pred.image_path.item() if isinstance(pred.image_path, list) else pred.image_path\n",
    "    pred_label = bool(pred.pred_label.item()) if hasattr(pred, 'pred_label') else None\n",
    "    image = pred.image\n",
    "\n",
    "    pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    norm_image_path = norm_path(image_path)\n",
    "    tags = dataset_dict_by_path.get(norm_image_path, {}).get(\"tags\", [])\n",
    "    class_name = dataset_dict_by_path.get(norm_image_path, {}).get(\"class_name\", \"\")\n",
    "    label = dataset_dict_by_path.get(norm_image_path, {}).get(\"label\", \"\")\n",
    "\n",
    "    sample = fo.Sample(filepath=image_path, ground_truth=fo.Classification(label=label))\n",
    "\n",
    "    if pred_score is not None:\n",
    "        sample[\"pred_score\"] = pred_score\n",
    "    if pred_label is not None:\n",
    "        sample[\"pred_label\"] = pred_label\n",
    "    if pred_mask is not None:\n",
    "        sample[\"pred_mask\"] = fo.Segmentation(mask=pred_mask.astype(np.uint8))\n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "mismatches = dataset.match((F(\"pred_score\") > 0.5) & (F(\"ground_truth.label\") == \"good\"))\n",
    "if len(mismatches) > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {len(mismatches)} potential mismatches\")\n",
    "# View this instead: fo.launch_app(mismatches)\n",
    "\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4b237",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Data Validation\n",
    "The screenshot above is static screenshot created automatically by FiftyOne.  In a live notebook environment, you can manipulate filters, and inspect images and predictions interactively.  This is useful to ensure that the model is making accurate predictions and to identify any potential issues with the data or the model.\n",
    "\n",
    "Future improvement could include versioning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c89de9",
   "metadata": {},
   "source": [
    "This is the end of the notebook. üëã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
