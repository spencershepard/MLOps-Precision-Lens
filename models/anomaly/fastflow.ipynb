{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc78e233",
   "metadata": {},
   "source": [
    "# FastFlow Anomaly Detection\n",
    "This notebook trains and evaluates a FastFlow model for anomaly detection.  Design tenants include high visibility on image data and model performance.  The goal is to provide a comprehensive framework for developing anomaly detection models, while explaining the code along the way.\n",
    "\n",
    "### Contents\n",
    "1. [Setup Variables](#setup-variables)\n",
    "2. [Setup MLFLow Logging](#setup-mlflow-logging)\n",
    "3. [Data Preparation](#data-preparation)\n",
    "4. [Training the Model](#training-the-model)\n",
    "5. [Model Evaluation](#model-evaluation) \n",
    "6. [Visualize a Single Prediction](#visualize-a-single-prediction)\n",
    "7. [Predict and Visualize the Dataset](#predict-and-visualize-the-dataset)\n",
    "8. [Data Validation](#-data-validation)\n",
    "9. [Save the Model](#save-the-model-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7e3c9",
   "metadata": {},
   "source": [
    "## Setup Variables ‚öôÔ∏è\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "load_dotenv(\"../../secrets.env\")\n",
    "load_dotenv(\"../../config.env\")\n",
    "\n",
    "ENVIRONMENT = os.getenv(\"ENVIRONMENT\", \"DEVELOPMENT\")\n",
    "\n",
    "logging.info(f\"Running in {ENVIRONMENT} environment\")\n",
    "\n",
    "if ENVIRONMENT == \"PRODUCTION\":\n",
    "    MLFLOW_URI = os.getenv('MLFLOW_URI')\n",
    "else:\n",
    "    MLFLOW_URI = \"http://mlflow-mlflow\"\n",
    "\n",
    "if MLFLOW_URI is None:\n",
    "    logging.warning(\"MLFLOW_URI not found in environment variables. MLFlow logging will not be enabled.\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket_name\", type=str, default=\"mvtechad\", help=\"S3 bucket name\")\n",
    "parser.add_argument(\"--category\", type=str, default=\"cable\", help=\"Dataset category (top-level directory in bucket)\")\n",
    "parser.add_argument(\"--cache_directory\", type=str, default=\"s3cache\", help=\"Cache directory (local path, will be created if not exists)\")\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=10, help=\"Maximum number of epochs for training\")\n",
    "args, _ = parser.parse_known_args()  # allows notebook execution\n",
    "\n",
    "bucket_name = args.bucket_name\n",
    "category = args.category\n",
    "cache_directory = args.cache_directory\n",
    "max_epochs = args.max_epochs\n",
    "\n",
    "logging.info(f\"Using bucket: {bucket_name}, category: {category}, cache directory: {cache_directory}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4485a",
   "metadata": {},
   "source": [
    "## Setup MLFLow Logging üìä\n",
    "MLFlow lets us log model parameters, metrics, and artifacts, making it easier to track experiments and reproduce results.  Additionally, it provides a centralized repository for managing and comparing different model versions.\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLFlow Logging\n",
    "\n",
    "if MLFLOW_URI is not None:\n",
    "    logging.info(f\"MLFLOW_URI found: {MLFLOW_URI}\")\n",
    "    import mlflow\n",
    "\n",
    "experiment_name = f\"{category}_fastflow_experiment\"\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045beced",
   "metadata": {},
   "source": [
    "## Data Preparation ‚¨áÔ∏è\n",
    "\n",
    "We'll use our s3data library to download/cache the data from our S3 bucket. This module expects a directory structure similar to the MVTechAD dataset. Then we'll use the anomalib Folder datamodule to hold our dataset.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3dataset import s3data\n",
    "from anomalib.data import Folder\n",
    "import os\n",
    "\n",
    "s3data.set_aws_config(\n",
    "    access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('AWS_REGION_NAME')\n",
    ")\n",
    "\n",
    "images, class_names, labels, tags = s3data.get_dataset(bucket_name=bucket_name, prefix=category, limit=1000, cache_dir=cache_directory)\n",
    "\n",
    "# Dynamically find anomaly directories under test/\n",
    "test_dir = os.path.join(cache_directory, category, 'test')\n",
    "abnormal_dirs = [\n",
    "    os.path.join('test', d)\n",
    "    for d in os.listdir(test_dir)\n",
    "    if os.path.isdir(os.path.join(test_dir, d)) and d != 'good'\n",
    "]\n",
    "print(f\"Found abnormal directories: {abnormal_dirs}\")\n",
    "\n",
    "mask_dir = os.path.join(cache_directory, category, 'ground_truth')\n",
    "mask_dirs = [\n",
    "    os.path.join('ground_truth', d)\n",
    "    for d in os.listdir(mask_dir)\n",
    "    if os.path.isdir(os.path.join(mask_dir, d))\n",
    "]\n",
    "\n",
    "print(f\"Found mask directories: {mask_dirs}\")\n",
    "\n",
    "datamodule = Folder(\n",
    "    name=category,\n",
    "    root=os.path.join(cache_directory, category),\n",
    "    normal_dir=os.path.join('train', 'good'),\n",
    "    mask_dir=mask_dirs,\n",
    "    normal_test_dir=os.path.join('test', 'good'),\n",
    "    abnormal_dir=abnormal_dirs,\n",
    ")\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "datamodule.prepare_data()\n",
    "\n",
    "print(f\"Setting up '{datamodule.name}' datasets...\")\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44301f3",
   "metadata": {},
   "source": [
    "### What is a DataLoader and what is a DataModule? ü§î\n",
    "\n",
    "**DataLoader:**\n",
    "\n",
    "- A PyTorch `DataLoader` is a utility that loads data from a dataset and serves it in batches to your model during training or inference.\n",
    "- It handles shuffling, batching, and parallel loading using multiple workers.\n",
    "- Example: `DataLoader(dataset, batch_size=32, shuffle=True)`\n",
    "\n",
    "**DataModule:**\n",
    "\n",
    "- A `DataModule` (from PyTorch Lightning or similar frameworks) is a higher-level abstraction that organizes all data-related steps for a project.\n",
    "- It encapsulates dataset preparation, setup, and provides ready-to-use DataLoaders for training, validation, and testing.\n",
    "- This helps keep data logic separate from model logic and makes experiments more reproducible.\n",
    "- Example: `datamodule.train_dataloader()` returns a DataLoader for training data.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for exploring the dataset only, and has no impact on training or the rest of the notebook\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training dataset size: {len(datamodule.train_data)}\")\n",
    "print(f\"Test dataset size: {len(datamodule.test_data)}\")\n",
    "\n",
    "datamodule_attribs = (attr for attr in dir(datamodule) if not attr.startswith(\"_\"))\n",
    "print(\"\\nDatamodule attributes:\")\n",
    "print(list(datamodule_attribs))\n",
    "\n",
    "# Get dataloaders\n",
    "train_loader = datamodule.train_dataloader()\n",
    "test_loader = datamodule.test_dataloader()\n",
    "\n",
    "print(\"\\nDataLoader batch sizes:\")\n",
    "print(f\"Training batch size: {train_loader.batch_size}\")\n",
    "print(f\"Test batch size: {test_loader.batch_size}\")\n",
    "\n",
    "# Get a item or batch\n",
    "data_item = next(iter(train_loader))\n",
    "print(f\"\\nThe dataloader provided the type {type(data_item)}\")\n",
    "data_item_attribs = (attr for attr in dir(data_item) if not attr.startswith(\"_\"))\n",
    "print(f\"Data item attributes: {list(data_item_attribs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06c76a",
   "metadata": {},
   "source": [
    "## Training the Model üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è\n",
    "- Callbacks are used to log metrics and model checkpoints during training. This allows for stopping training early if the model is not improving.\n",
    "- Model checkpoints allow you to resume training from a specific point or to evaluate the model's performance at different stages.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models import Fastflow\n",
    "from anomalib.engine import Engine\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Custom callback to log checkpoints to MLflow\n",
    "import lightning.pytorch as pl\n",
    "class MLflowCheckpointLogger(pl.callbacks.Callback):\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        # log the train loss\n",
    "        mlflow.log_metric(\"train_loss\", trainer.callback_metrics[\"train_loss\"].item())\n",
    "        # logging.info(\"Saving checkpoint to MLflow\")\n",
    "        # # Find the last checkpoint path from ModelCheckpoint callback\n",
    "        # for cb in trainer.callbacks:\n",
    "        #     if isinstance(cb, ModelCheckpoint):\n",
    "        #         ckpt_path = cb.last_model_path\n",
    "        #         if ckpt_path and os.path.exists(ckpt_path):\n",
    "        #             mlflow.log_artifact(ckpt_path)\n",
    "\n",
    "model = Fastflow(\n",
    "    backbone=\"resnet18\",  # or resnet50\n",
    "    pre_trained=True,\n",
    "    evaluator=True\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"train_loss\", \n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"epoch_{epoch}_step_{step}\",\n",
    "        auto_insert_metric_name=False,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"train_loss\",\n",
    "        patience=3,\n",
    "    ),\n",
    "    MLflowCheckpointLogger(),\n",
    "]\n",
    "\n",
    "engine = Engine(\n",
    "    callbacks=callbacks,\n",
    "    logger=None,  # Disable AnomalibMLFlowLogger since there was a bug with artifact logging. We'll use MLflow directly\n",
    "    accelerator=\"auto\", \n",
    "    devices=1, \n",
    "    max_epochs=max_epochs,\n",
    ")\n",
    "mlflow.end_run() #in case there is a run already active \n",
    "mlflow_run = mlflow.start_run(tags={\"model\": \"fastflow\"}) \n",
    "mlflow.log_param(\"category\", category)\n",
    "mlflow.log_param(\"backbone\", \"resnet18\")\n",
    "mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f83bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Training the model\")\n",
    "\n",
    "engine.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Evaluating model\")\n",
    "\n",
    "results = engine.validate(model=model, datamodule=datamodule)\n",
    "\n",
    "metrics_dict = {}\n",
    "if isinstance(results, list) and results and isinstance(results[0], dict):\n",
    "    metrics_dict['image_AUROC'] = float(results[0].get('image_AUROC', float('nan')))\n",
    "    metrics_dict['pixel_AUROC'] = float(results[0].get('pixel_AUROC', float('nan')))\n",
    "\n",
    "if metrics_dict:\n",
    "    logging.info(f\"Logging metrics to MLflow: {metrics_dict}\")\n",
    "    mlflow.log_metrics(metrics_dict)\n",
    "else:\n",
    "    logging.warning(\"No metrics found to log. Is the evaluator working properly?\")\n",
    "\n",
    "logging.info(\"Training and evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630281c",
   "metadata": {},
   "source": [
    "# Visualize a Single Prediction üñºÔ∏è\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9183932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# TODO: Implement batching, and create a plots for multiple sample.  These will be logged to MLflow with the model.\n",
    "\n",
    "# pandas dataframe\n",
    "test_samples = datamodule.test_data.samples\n",
    "\n",
    "# Get a random row from the samples\n",
    "random_sample = test_samples.sample(1)\n",
    "path = str(random_sample.iloc[0][\"image_path\"])\n",
    "\n",
    "predictions = engine.predict(model=model, data_path=path)  #You can also point to a folder with image or a single image instead of passing a dataset\n",
    "\n",
    "def plot_results(data_sample):\n",
    "    path = str(data_sample.iloc[0][\"image_path\"])\n",
    "\n",
    "\n",
    "    original_img = Image.open(path)\n",
    "    mask = Image.open(str(random_sample.iloc[0][\"mask_path\"]))\n",
    "\n",
    "    # Extract the first prediction result\n",
    "    pred = predictions[0]\n",
    "\n",
    "    print(\n",
    "        f'Image Shape: {pred.image.shape},\\n'\n",
    "        f'Anomaly Map Shape: {pred.anomaly_map.shape}, \\n'\n",
    "        f'Predicted Mask Shape: {pred.pred_mask.shape}',\n",
    "    )\n",
    "\n",
    "    # Extract label and tag from the sample DataFrame\n",
    "    label = random_sample.iloc[0][\"label\"] if \"label\" in random_sample.columns else None\n",
    "    tag = random_sample.iloc[0][\"tag\"] if \"tag\" in random_sample.columns else None\n",
    "\n",
    "    # Create subplots for image, anomaly map, and predicted mask\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    img = pred.image\n",
    "    print(f\"Image Shape: {img.shape}\")\n",
    "    print(f\"Image dims: {img.dim()}\")\n",
    "    if img.dim() == 4:\n",
    "        # Batch of images, select the first one\n",
    "        img = img[0]\n",
    "    if img.dim() == 3:\n",
    "        img = img.permute(1, 2, 0).cpu().numpy()  # reorders the dimensions of the tensor from `[C, H, W]` to `[H, W, C]`\n",
    "    elif img.dim() == 2:\n",
    "        img = img.cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape after batch selection: {img.shape}\")\n",
    "\n",
    "    anomaly_map = pred.anomaly_map\n",
    "    if anomaly_map.dim() > 2:\n",
    "        anomaly_map = anomaly_map[0]\n",
    "    anomaly_map = anomaly_map.squeeze().cpu().numpy()\n",
    "\n",
    "    pred_mask = pred.pred_mask\n",
    "    if pred_mask.dim() > 2:\n",
    "        pred_mask = pred_mask[0]\n",
    "    pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "    title_str = \"Original Image\"\n",
    "    title_str += f\"\\nLabel: {label} | Tag: {tag}\"\n",
    "\n",
    "    axs[0].imshow(original_img)\n",
    "    axs[0].set_title(title_str)\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(anomaly_map, cmap=\"jet\")\n",
    "    axs[1].set_title(\"Anomaly Map\")\n",
    "    axs[2].imshow(pred_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[3].imshow(mask, cmap=\"gray\")\n",
    "    axs[3].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "    for ax in axs: ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Path: {path}\")\n",
    "    print(f\"Pred Score: {pred.pred_score}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "figure = plot_results(random_sample)\n",
    "figure.savefig(\"fastflow_prediction.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cc9c2",
   "metadata": {},
   "source": [
    "## Predict and Visualize the Dataset üìÖ\n",
    "We will use the trained FastFlow model to make predictions on the entire dataset and visualize the results using Voxel FiftyOne. With this interactive tool, we can explore the model's predictions, view the original images, and analyze the results.  The goal is to gain insights into the model's performance and identify any potential areas for improvement, both in the model and the dataset.\n",
    "\n",
    "<small>[Back to Top](#contents)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2563d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.core.labels import Classification, Segmentation\n",
    "import numpy as np\n",
    "\n",
    "images, class_names, labels, tags = s3data.get_dataset(bucket_name=bucket_name, prefix=category, limit=1000, cache_dir=cache_directory)\n",
    "# Normalize all image paths to absolute paths for consistent lookup\n",
    "def norm_path(p):\n",
    "    return os.path.normcase(os.path.abspath(p))\n",
    "\n",
    "dataset_dict_by_path = {\n",
    "    norm_path(img): {\n",
    "        \"image\": img,\n",
    "        \"class_name\": class_names[i],\n",
    "        \"label\": labels[i],\n",
    "        \"tags\": tags[i]\n",
    "    }\n",
    "    for i, img in enumerate(images)\n",
    "}\n",
    "\n",
    "print(f\"Total images to process: {len(images)}\")\n",
    "# Create FO dataset from images, class_names, labels, and tags\n",
    "name = \"my-labelled-dataset\"\n",
    "if fo.dataset_exists(name):\n",
    "    fo.delete_dataset(name)\n",
    "dataset = fo.Dataset(name=name)\n",
    "\n",
    "predictions = engine.predict(model=model, datamodule=datamodule)\n",
    "print(f\"Batches of predictions made: {len(predictions)}\")\n",
    "# flatten batches\n",
    "predictions = [item for batch in predictions for item in batch]\n",
    "print(f\"Total predictions made: {len(predictions)}\")\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "\n",
    "    ## Important prediction attributes:\n",
    "    # image (Image)\n",
    "    # anomaly_map (Mask)\n",
    "    # pred_score (tensor([[0.09]]))\n",
    "    # pred_mask (Mask)\n",
    "    # pred_label (tensor[False]))\n",
    "    # image_path (['C:/...'])\n",
    "\n",
    "    anomaly_map = pred.anomaly_map\n",
    "    pred_score = float(pred.pred_score) if hasattr(pred, 'pred_score') else None\n",
    "    pred_mask = pred.pred_mask\n",
    "    image_path = pred.image_path.item() if isinstance(pred.image_path, list) else pred.image_path\n",
    "    pred_label = bool(pred.pred_label.item()) if hasattr(pred, 'pred_label') else None\n",
    "    image = pred.image\n",
    "\n",
    "    pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    norm_image_path = norm_path(image_path)\n",
    "    tags = dataset_dict_by_path.get(norm_image_path, {}).get(\"tags\", [])\n",
    "    class_name = dataset_dict_by_path.get(norm_image_path, {}).get(\"class_name\", \"\")\n",
    "    label = dataset_dict_by_path.get(norm_image_path, {}).get(\"label\", \"\")\n",
    "\n",
    "    sample = fo.Sample(filepath=image_path, ground_truth=fo.Classification(label=label))\n",
    "\n",
    "    if pred_score is not None:\n",
    "        sample[\"pred_score\"] = pred_score\n",
    "    if pred_label is not None:\n",
    "        sample[\"pred_label\"] = pred_label\n",
    "    if pred_mask is not None:\n",
    "        sample[\"pred_mask\"] = fo.Segmentation(mask=pred_mask.astype(np.uint8))\n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4b237",
   "metadata": {},
   "source": [
    "# üïµÔ∏è Data Validation\n",
    "The screenshot above is static screenshot created automatically by FiftyOne.  In a live notebook environment, you can manipulate filters, and inspect images and predictions interactively.  This is useful to ensure that the model is making accurate predictions and to identify any potential issues with the data or the model.\n",
    "\n",
    "Future improvement could include versioning the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66efac7",
   "metadata": {},
   "source": [
    "# Save the Model üíæ\n",
    "Log the model to MLFlow.  In the MLFlow UI we can promote the model to production, or set up automations to promote it automatically based on our collected metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "logging.info(\"Saving the model\")\n",
    "\n",
    "model_name = f\"{category}_fastflow\"\n",
    "model_metrics = {}\n",
    "if 'metrics_dict' in locals() and metrics_dict:\n",
    "    if 'image_AUROC' in metrics_dict:\n",
    "        model_metrics['image_AUROC'] = metrics_dict['image_AUROC']\n",
    "    if 'pixel_AUROC' in metrics_dict:\n",
    "        model_metrics['pixel_AUROC'] = metrics_dict['pixel_AUROC']\n",
    "\n",
    "dataset_info = {\n",
    "    \"train_size\": len(datamodule.train_data),\n",
    "    \"test_size\": len(datamodule.test_data),\n",
    "    \"category\": category,\n",
    "    \"bucket\": bucket_name\n",
    "}\n",
    "\n",
    "# Log the prediction image as an artifact\n",
    "if os.path.exists(\"fastflow_prediction.png\"):\n",
    "    mlflow.log_artifact(\"fastflow_prediction.png\", \"predictions\")\n",
    "\n",
    "logged_model = mlflow.pytorch.log_model(\n",
    "    model,\n",
    "    name=model_name, #artifact_path is depreciated\n",
    "    metadata={\n",
    "        'image_AUROC': model_metrics.get('image_AUROC'),\n",
    "        'pixel_AUROC': model_metrics.get('pixel_AUROC'),\n",
    "        'dataset_train_size': dataset_info.get('train_size'),\n",
    "        'dataset_test_size': dataset_info.get('test_size'),\n",
    "        'dataset_bucket_name': dataset_info.get('bucket'),\n",
    "        'dataset_category': dataset_info.get('category')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Register the model in MLflow Model Registry\n",
    "registered_model = mlflow.register_model(\n",
    "    logged_model.model_uri,\n",
    "    f\"{category}_fastflow_model\"\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(f\"Model and artifacts registered as: {registered_model.name} (Version: {registered_model.version})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close_app()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c89de9",
   "metadata": {},
   "source": [
    "This is the end of the notebook. üëã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
